# Redis 三主三从集群故障场景综合总结

## 一、基础概念说明

### 1.1 关键机制

```
┌────────────────────────────────────────────────────────┐
│              Redis 集群核心机制                          │
└────────────────────────────────────────────────────────┘

机制1: 槽位分配 (Slot Assignment)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
- Redis 集群有 16384 个槽位 (0-16383)
- 每个 Key 通过 CRC16(key) % 16384 映射到槽位
- 每个主节点负责一部分槽位
- 三主三从架构:
  Master A: 槽位 0-5460      (约 5461 个)
  Master B: 槽位 5461-10922  (约 5461 个)
  Master C: 槽位 10923-16383 (约 5462 个)

重要: 槽位归属信息存储在集群配置中,不是数据!


机制2: 主从复制 (Replication)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
客户端写入流程:
1. Client → Master: SET Key1 "value"
2. Master 执行写入
3. Master 返回 OK 给客户端
4. Master 异步发送复制命令到 Slave
5. Slave 执行相同的写入命令

特点:
- 异步复制: 主节点不等从节点确认
- 复制延迟: 通常 5-10ms
- 数据方向: 主 → 从 (单向)


机制3: 持久化 (Persistence)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
AOF (Append Only File):
- 记录每个写命令
- appendfsync 配置:
  always:   每次写入立即 fsync (最安全,最慢)
  everysec: 每秒 fsync 一次 (推荐,平衡性能和安全)
  no:       由 OS 决定 (最快,最不安全)

RDB (Snapshot):
- 定期保存内存快照
- save 配置决定触发时机
- 例: save 900 1 (900秒内至少1个key变化)

重要: 持久化是每个节点独立的!


机制4: 故障转移 (Failover)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
触发条件:
1. 主节点被标记为 FAIL
2. 有健康的从节点
3. 从节点能获得多数派投票

选举规则:
- 需要 ⌊N/2⌋ + 1 票 (N=主节点总数)
- 三主集群需要 2 票
- 投票者: 存活的主节点

时间线:
T0:      主节点宕机
T0+15s:  被标记为 FAIL (cluster-node-timeout)
T0+16s:  从节点晋升完成


机制5: 集群完整性检查
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
cluster-require-full-coverage yes (默认):
- 所有 16384 槽位必须有主节点
- 任何槽位缺失 → 整个集群 CLUSTERDOWN
- 保证数据完整性

cluster-require-full-coverage no:
- 允许部分槽位缺失
- 可用槽位仍可访问
- 不可用槽位返回错误
```

### 1.2 重要概念澄清

```
┌────────────────────────────────────────────────────────┐
│              Key 迁移 vs 槽位归属变更                    │
└────────────────────────────────────────────────────────┘

常见误解: "主节点宕机后,Key 会迁移到其他节点"
实际情况: "槽位归属变更,但 Key 不会物理迁移"

详细说明:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

场景: Master B 宕机,Slave B1 晋升

宕机前:
┌─────────────────────────────────────────┐
│ Master B (7001):                        │
│ - 内存中存储: Key2, Key5, Key8            │
│ - 磁盘持久化: Key2, Key5, Key8 (可能)      │
│ - 负责槽位: 5461-10922                    │
└─────────────────────────────────────────┘
        │
        │ 实时复制
        ↓
┌─────────────────────────────────────────┐
│ Slave B1 (7004):                        │
│ - 内存中存储: Key2, Key5, Key8 (副本)     │
│ - 磁盘持久化: Key2, Key5, Key8 (可能)     │
│ - 角色: 从节点                           │
└─────────────────────────────────────────┘

Master B 宕机后:
┌─────────────────────────────────────────┐
│ Master B (7001): 离线 ❌                 │
│ - 内存数据: 不可访问                       │ 
│ - 磁盘数据: 仍在磁盘上,但不可访问            │
└─────────────────────────────────────────┘

Slave B1 晋升:
┌─────────────────────────────────────────┐
│ 新 Master B1 (7004):                    │
│ - 内存中存储: Key2, Key5, Key8 (原有)     │
│ - 磁盘持久化: Key2, Key5, Key8 (原有)     │
│ - 负责槽位: 5461-10922 ✅ (接管)          │
│ - 角色变更: 从节点 → 主节点                 │
└─────────────────────────────────────────┘

关键点:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Key 本身没有"迁移"
   - Slave B1 本来就有这些 Key 的副本
   - 只是角色从"备份"变成"主要"

2. 发生的是"槽位归属变更"
   - 槽位 5461-10922 的负责人: B → B1
   - 集群配置更新,通知所有节点

3. 客户端重定向
   - 客户端请求 Key5 时
   - 集群告知: 槽位现在由 B1 负责
   - 客户端更新本地路由表

4. 数据未物理移动
   - 没有网络传输 Key 的过程
   - 没有跨节点数据复制
   - 只是"这些 Key 现在归谁管"的声明变化
```

---

## 二、场景1: 停掉一个主节点

### 2.1 开启持久化 (appendfsync everysec)

```
┌────────────────────────────────────────────────────────┐
│    场景1A: 停掉 Master B (开启持久化 appendfsync everysec)│
└────────────────────────────────────────────────────────┘

初始架构:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A (0-5460)     Master B (5461-10922)   Master C (10923-16383)
    ↓                       ↓                        ↓
Slave A1              Slave B1                   Slave C1


T0: Master B 突然宕机
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

宕机瞬间数据状态:
┌─────────────────────────────────────────┐
│ Master B (内存):                        │
│ - Key100: "val100" ✅                   │
│ - Key200: "val200" ✅                   │
│ - Key300: "val300" ⚠️ (刚写入)           │
│                                         │
│ Master B (AOF 文件):                    │
│ - Key100: 已 fsync ✅                   │
│ - Key200: 在缓冲区 ⚠️ (1秒内)             │
│ - Key300: 在缓冲区 ⚠️ (未fsync)           │
│                                         │
│ Slave B1 (内存):                        │
│ - Key100: "val100" ✅ (已复制)           │
│ - Key200: "val200" ⚠️ (可能在传输中)      │
│ - Key300: 无 ❌ (复制延迟,未收到)          │
└─────────────────────────────────────────┘


T0 ~ T0+15s: 故障检测期
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
- Slave B1 检测到主节点失联
- 其他节点通过 PING 发现 Master B 不响应
- Gossip 协议传播故障信息
- Master B 被标记为 FAIL

集群状态: 
- 槽位 5461-10922 无主节点
- cluster-require-full-coverage yes → CLUSTERDOWN ❌
- 客户端所有请求失败 ❌

Key 状态:
- 没有任何 Key 迁移发生
- Master B 的数据在其磁盘上(不可访问)
- Slave B1 的数据在其内存中(等待晋升)


T0+15s ~ T0+16s: 故障转移
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Slave B1 发起选举:
1. 广播 FAILOVER_AUTH_REQUEST
2. Master A 投票: 赞成 ✅
3. Master C 投票: 赞成 ✅
4. 获得 2 票,选举成功

Slave B1 晋升为主节点:
┌─────────────────────────────────────────┐
│ 新 Master B1:                           │
│ - 角色: Slave → Master                  │
│ - 负责槽位: 5461-10922 ✅               │
│ - 数据: 使用自己内存中的数据               │
│   ├─ Key100: ✅ 存在                    │
│   ├─ Key200: ⚠️ 可能存在                 │
│   └─ Key300: ❌ 不存在 (未复制到)         │
│ - 广播槽位归属变更                         │
└─────────────────────────────────────────┘

关键: 没有 Key 从 Master B "迁移"到 Slave B1
      Slave B1 使用自己已有的数据副本


T0+16s: 集群恢复正常
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
集群状态: OK ✅
所有槽位有主节点覆盖
客户端请求恢复正常


T0+1小时: 旧 Master B 恢复
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
旧 Master B 启动:
1. 从 AOF 文件加载数据
   ├─ Key100: ✅ 恢复 (已fsync)
   ├─ Key200: ⚠️ 可能恢复 (在1秒窗口内)
   └─ Key300: ❌ 丢失 (未fsync)

2. 加入集群,发现 Slave B1 已晋升
   - config epoch 对比: B1 更新(6) > B(5)
   - 结论: B1 是合法的主节点

3. 自动降级为从节点 (Slave B2)
   - 执行 SLAVEOF B1
   - 清空自己的数据
   - 从新 Master B1 全量同步

4. 全量同步过程:
   新 Master B1              旧 Master B (Slave B2)
   ┌─────────────┐          ┌─────────────┐
   │ Key100 ✅   │          │ 清空数据...   │
   │ Key200 ⚠️   │          │             │
   │ (无Key300)  │          │             │
   └──────┬──────┘          └──────┬──────┘
          │                        │
          │  生成 RDB 快照          │
          ├───────────────────────►│ 加载 RDB
          │                        │
          │  发送增量命令           │
          ├───────────────────────►│
          │                        │
          │  后续实时复制           │
          ├───────────────────────►│

最终数据状态:
┌─────────────────────────────────────────┐
│ 新 Master B1:          Slave B2:        │
│ - Key100 ✅           - Key100 ✅       │
│ - Key200 ⚠️           - Key200 ⚠️       │
│ - Key300 ❌           - Key300 ❌       │
└─────────────────────────────────────────┘

Key300 永久丢失原因:
1. Master B 停机时未 fsync 到磁盘
2. Slave B1 停机时未收到复制命令
3. Slave B1 晋升后没有 Key300
4. 旧 Master B 恢复后从 B1 同步,也得不到


最终架构:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A (0-5460)   新Master B1 (5461-10922)  Master C (10923-16383)
    ↓                       ↓                          ↓
Slave A1                Slave B2                   Slave C1
                       (原Master B)
```

#### 数据丢失总结

```
┌────────────────────────────────────────────────────────┐
│        场景1A 数据丢失分析 (appendfsync everysec)         │
└────────────────────────────────────────────────────────┘

丢失窗口1: AOF 持久化延迟
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
时间窗口: 最后 1 秒的写入
示例: T-0.5s 的 Key300
原因: 数据在 AOF 缓冲区,未 fsync
结果: ❌ 永久丢失

丢失窗口2: 主从复制延迟
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
时间窗口: 最后 5-10ms 的写入
示例: T-5ms 的 Key200 (可能)
原因: 复制命令在传输中
结果: ⚠️ 可能丢失

总数据丢失: 约 1 秒内的数据
影响范围: 极少量 Key (取决于写入 QPS)
不可用时间: ~15-17 秒
```

### 2.2 未开启持久化

```
┌────────────────────────────────────────────────────────┐
│        场景1B: 停掉 Master B (未开启持久化)               │
└────────────────────────────────────────────────────────┘

配置:
appendonly no
save ""

T0: Master B 突然宕机
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

宕机瞬间数据状态:
┌─────────────────────────────────────────┐
│ Master B (内存):                        │
│ - Key100, Key200, Key300 (全在内存)      │
│                                         │
│ Master B (磁盘):                        │
│ - 无任何持久化文件 ❌                      │
│                                         │
│ Slave B1 (内存):                        │
│ - Key100: ✅ (已复制)                    │
│ - Key200: ✅ (已复制)                    │
│ - Key300: ❌ (复制延迟,未收到)            │
└─────────────────────────────────────────┘


T0+16s: Slave B1 晋升
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
新 Master B1 的数据:
- 使用自己内存中的副本数据
- Key100: ✅ 存在
- Key200: ✅ 存在  
- Key300: ❌ 不存在 (未复制到)

集群恢复正常 ✅


T0+1小时: 旧 Master B 恢复
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
旧 Master B 启动:
1. 尝试从磁盘加载数据
   - 无持久化文件 ❌
   - 启动后内存为空!

2. 加入集群,发现 Slave B1 已晋升
   - 自动降级为从节点

3. 从新 Master B1 全量同步
   新 Master B1              旧 Master B (空)
   ┌─────────────┐          ┌─────────────┐
   │ Key100 ✅   │          │   空!        │
   │ Key200 ✅   │          │             │
   │ (无Key300)  │          │             │
   └──────┬──────┘          └──────┬──────┘
          │                        │
          │  发送 RDB 快照          │
          ├───────────────────────►│ 加载数据
          │                        │
          │                        │
          ↓                        ↓
   保持原有数据                从 B1 复制所有数据

最终数据状态:
- Key100: ✅ 存在 (从 B1 同步)
- Key200: ✅ 存在 (从 B1 同步)
- Key300: ❌ 不存在 (B1 也没有)

数据丢失总结:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
丢失窗口: 仅主从复制延迟窗口 (5-10ms)
原因: Master B 宕机时,部分数据未复制到 Slave B1
结果: 比开启持久化丢失更少! ✅

关键洞察:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
无持久化场景下,数据丢失反而更少!
原因: 
- 有持久化: 丢失"持久化延迟+复制延迟"
- 无持久化: 只丢失"复制延迟"

但注意: 这仅在有健康从节点的情况下成立!
```

---

## 三、场景2: 停掉两个主节点

### 3.1 开启持久化

```
┌────────────────────────────────────────────────────────┐
│   场景2A: 停掉 Master A & B (开启持久化)                  │
└────────────────────────────────────────────────────────┘

T0: Master A 和 Master B 同时宕机
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

集群状态:
Master A ❌              Master B ❌              Master C ✅
槽位: 0-5460            槽位: 5461-10922         槽位: 10923-16383
    ↓                       ↓                        ↓
Slave A1 ⚠️             Slave B1 ⚠️              Slave C1 ✅
(想晋升)                (想晋升)                 (正常)


T0+15s: 尝试故障转移但失败
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Slave A1 和 B1 都想晋升:
┌──────────────────────────────────────────┐
│ Slave A1 发起选举:                        │
│ ├─ 需要票数: 2 票                         │
│ ├─ Master A: 宕机,无法投票 ❌              │
│ ├─ Master B: 宕机,无法投票 ❌              │
│ ├─ Master C: 可以投票 ✅ (1票)            │
│ └─ 结果: 票数不足 ❌                       │
│                                          │
│ Slave B1 发起选举:                        │
│ ├─ 需要票数: 2 票                         │
│ ├─ 同样只能获得 Master C 的 1 票           │
│ └─ 结果: 票数不足 ❌                       │
└──────────────────────────────────────────┘

集群状态: CLUSTERDOWN ❌
原因: 槽位 0-10922 无主节点覆盖


关键点: 没有任何 Key 迁移!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A 的数据: 在 A 的磁盘上 (不可访问)
Master B 的数据: 在 B 的磁盘上 (不可访问)
Slave A1 的数据: 在 A1 的内存中 (等待,但无法晋升)
Slave B1 的数据: 在 B1 的内存中 (等待,但无法晋升)

槽位状态:
- 0-5460:      仍属于 Master A (但 A 离线)
- 5461-10922:  仍属于 Master B (但 B 离线)
- 10923-16383: 属于 Master C ✅


T0+1小时: 恢复 Master A 和 Master B
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Master A 恢复:
1. 从 AOF/RDB 加载数据
   - 恢复停机前约 1 秒之前的数据
   - 最后 1 秒数据可能丢失 ⚠️

2. 重新加入集群
   - 声明负责槽位 0-5460
   - 槽位归属没有改变 ✅

3. 与 Slave A1 重建复制关系
   - Slave A1 仍是其从节点
   - 开始实时复制

Master B 恢复:
1. 从 AOF/RDB 加载数据
   - 恢复停机前约 1 秒之前的数据

2. 重新加入集群
   - 声明负责槽位 5461-10922

3. 与 Slave B1 重建复制关系

集群状态恢复: OK ✅


数据最终状态:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A: 丢失最后 ~1秒数据 ⚠️
Master B: 丢失最后 ~1秒数据 ⚠️
Master C: 无影响 ✅

关键: 数据没有"迁移",只是重新上线恢复了访问


最终架构: 与初始架构相同
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A (0-5460)     Master B (5461-10922)   Master C (10923-16383)
    ↓                       ↓                        ↓
Slave A1              Slave B1                   Slave C1
```

#### 数据丢失总结

```
┌────────────────────────────────────────────────────────┐
│            场景2A 数据丢失分析                            │
└────────────────────────────────────────────────────────┘

数据丢失:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A: 最后 ~1 秒数据 ⚠️ (持久化窗口)
Master B: 最后 ~1 秒数据 ⚠️ (持久化窗口)

为什么没有复制延迟窗口的丢失?
因为两个主节点恢复后,使用自己的持久化文件,
而不是从从节点同步数据。

不可用时间: 直到人工恢复节点 (可能很长)

关键区别:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
场景1 (1个主节点宕机):
- 从节点自动晋升 ✅
- 数据来自从节点
- 丢失: 持久化窗口 + 复制窗口

场景2 (2个主节点宕机):
- 无法自动恢复 ❌
- 数据来自主节点自己的持久化文件
- 丢失: 仅持久化窗口
```

### 3.2 未开启持久化

```
┌────────────────────────────────────────────────────────┐
│     场景2B: 停掉 Master A & B (未开启持久化)               │
└────────────────────────────────────────────────────────┘

T0: Master A 和 Master B 同时宕机
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

数据状态:
Master A (内存): 所有数据在内存 ❌
Master A (磁盘): 无持久化文件 ❌

Master B (内存): 所有数据在内存 ❌
Master B (磁盘): 无持久化文件 ❌

Slave A1 (内存): A 的副本数据 ✅ (除复制延迟窗口)
Slave B1 (内存): B 的副本数据 ✅ (除复制延迟窗口)


T0+15s: 故障转移失败
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
原因同上,票数不足

集群状态: CLUSTERDOWN ❌

关键状态:
- Master A 和 B 的数据完全丢失 (在内存中) ❌❌
- Slave A1 和 B1 有副本数据,但无法晋升 ⚠️
- 数据实际上存在(在 Slave 内存中),但集群无法使用


T0+1小时: 恢复 Master A 和 Master B
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Master A 恢复:
1. 尝试加载数据
   - 无持久化文件 ❌
   - 启动后内存为空

2. 加入集群
   - 声明负责槽位 0-5460
   - 但数据为空! ❌

3. 与 Slave A1 建立复制关系
   - 主节点为空
   - 从节点有数据 ⚠️
   
问题: 主节点为空,会怎样?
┌──────────────────────────────────────────┐
│ 正常情况: 从节点从主节点同步数据              │
│ 异常情况: 主节点为空,从节点有数据             │
│                                          │
│ Redis 的行为:                             │
│ 主节点空启动后,从节点会:                     │
│ 1. 与主节点建立连接                         │
│ 2. 发现主节点是新的/重启的                   │
│ 3. 执行全量同步                            │
│ 4. 主节点发送空的 RDB                      │
│ 5. 从节点清空自己的数据 ❌❌❌               │
│ 6. 从节点变成空的!                         │
│                                          │
│ 结果: 数据被主节点的"空"覆盖了!              │
└──────────────────────────────────────────┘

最终数据状态:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Master A:  空 ❌
Slave A1:  空 ❌ (被主节点覆盖)

Master B:  空 ❌
Slave B1:  空 ❌ (被主节点覆盖)

Master C:  正常 ✅
Slave C1:  正常 ✅

灾难结果: 槽位 0-10922 的所有数据全部丢失! ❌❌❌
```

#### 灾难性场景详解

```
┌────────────────────────────────────────────────────────┐
│          为什么从节点的数据也丢失了?                        │
└────────────────────────────────────────────────────────┘

时间线详解:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

T0:      Master A 和 B 宕机
         Slave A1 和 B1 的内存中有副本数据 ✅

T0+1h:   Master A 启动 (无持久化,空数据)
         
T0+1h+1s: Slave A1 与 Master A 建立连接
         
         Master A (空)           Slave A1 (有数据)
         ┌─────────┐             ┌──────────────┐
         │  空!    │             │ Key1: val1   │
         │         │             │ Key4: val4   │
         └────┬────┘             │ Key7: val7   │
              │                  │ ... 等等     │
              │                  └──────┬───────┘
              │                         │
              │  ① 我是主节点,我的       │
              │     run_id 是 xyz123    │
              │◄────────────────────────┤
              │                         │
              │  ② 我需要与你全量同步      │
              ├────────────────────────►│
              │                         │
              │  ③ 发送 RDB (空快照)     │
              ├────────────────────────►│
              │                         │  ④ 清空内存
              │                         │  ⑤ 加载空 RDB
              │                         │
              │  ⑥ 后续实时复制          │
              ├────────────────────────►│
              │     (但没有任何数据)      │
              │                         │
              ↓                         ↓
         仍然为空                    变成空了! ❌

为什么不是从节点把数据"反向"同步给主节点?
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
因为 Redis 主从复制是单向的:
- 主节点是唯一的真理来源 (Source of Truth)
- 从节点必须与主节点保持一致
- 从节点不能"逆向"同步数据给主节点
- 这是 Redis 复制协议的基本设计

结果: 从节点的数据被主节点的"空"覆盖了 ❌❌❌
```

---

## 四、场景3: 停掉一主一从 (同一组)

### 4.1 开启持久化

```
┌────────────────────────────────────────────────────────┐
│  场景3A: 停掉 Master B & Slave B1 (开启持久化)            │
└────────────────────────────────────────────────────────┘

T0: Master B 和 Slave B1 同时宕机
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

集群状态:
Master A ✅              Master B ❌              Master C ✅
    ↓                   [整组下线]                   ↓
Slave A1 ✅             Slave B1 ❌              Slave C1 ✅

槽位状态:
- 0-5460:      Master A ✅
- 5461-10922:  无节点! ❌ (主从都宕机)
- 10923-16383: Master C ✅


T0+15s: 无法故障转移
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

原因: 没有健康的从节点可以晋升
┌──────────────────────────────────────────┐
│ Master B: 宕机 ❌                         │
│ Slave B1: 宕机 ❌                         │
│                                          │
│ 无可用的从节点来接管槽位 5461-10922          │
│ 故障转移根本无法启动!                       │
└──────────────────────────────────────────┘

集群状态: CLUSTERDOWN ❌
不可用时间: 直到至少一个节点恢复


恢复子场景A: 只恢复 Master B
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Master B 启动:
1. 从 AOF/RDB 加载数据
   - 恢复约 1 秒前的数据
   - 最后 1 秒数据丢失 ⚠️

2. 重新加入集群
   - 声明负责槽位 5461-10922
   - 集群检测到所有槽位覆盖
   - 集群状态: OK ✅

3. 等待 Slave B1 恢复
   - 当前无从节点
   - 存在单点故障风险 ⚠️

最终状态:
Master A (0-5460)     Master B (5461-10922)   Master C (10923-16383)
    ↓                    (无从节点⚠️)               ↓
Slave A1                                       Slave C1

数据丢失: Master B 最后 ~1 秒数据


恢复子场景B: 只恢复 Slave B1
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Slave B1 启动:
1. 从 AOF/RDB 加载数据
   - 恢复副本数据
   - 可能缺少复制延迟窗口数据 ⚠️

2. 发现主节点 Master B 仍宕机

3. 等待 ~15 秒后发起选举
   ├─ Master A 投票: 赞成 ✅
   ├─ Master C 投票: 赞成 ✅
   └─ 获得 2 票,选举成功 ✅

4. 晋升为新主节点 (Master B1)
   - 接管槽位 5461-10922
   - 集群状态: OK ✅

最终状态:
Master A (0-5460)   新Master B1 (5461-10922)  Master C (10923-16383)
    ↓                    (无从节点⚠️)               ↓
Slave A1                                       Slave C1

数据丢失: 复制延迟窗口数据 (~10ms)


恢复子场景C: 同时恢复 Master B 和 Slave B1
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

情况1: Master B 先启动完成
- Master B 接管槽位
- 集群恢复
- Slave B1 后续加入,成为从节点
- 恢复完整三主三从架构 ✅

情况2: Slave B1 先启动完成
- Slave B1 晋升为主节点
- Master B 后续加入,自动降级为从节点
- 角色互换,但架构完整 ✅

情况3: 同时启动,Master B 够快
- Master B 抢先接管槽位
- Slave B1 不发起选举,直接复制
- 保持原有角色 ✅
```

### 4.2 未开启持久化

```
┌────────────────────────────────────────────────────────┐
│   场景3B: 停掉 Master B & Slave B1 (未开启持久化)         │
└────────────────────────────────────────────────────────┘

T0: Master B 和 Slave B1 同时宕机
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

数据状态:
Master B:  所有数据在内存 ❌ 无持久化 ❌
Slave B1:  所有数据在内存 ❌ 无持久化 ❌

两者宕机后,槽位 5461-10922 的所有数据:
- 只存在于已关闭的内存中
- 没有任何持久化副本
- 事实上已丢失! ❌❌


T0+15s: 集群 CLUSTERDOWN
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
同场景 3A


恢复子场景A: 只恢复 Master B (灾难!)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Master B 启动:
1. 尝试加载数据
   - 无持久化文件 ❌
   - 启动后为空

2. 加入集群
   - 声明负责槽位 5461-10922
   - 但数据全部丢失! ❌❌❌

3. 集群状态恢复: OK ✅
   但槽位 5461-10922 为空集!

灾难结果: 该槽位段所有数据永久丢失 ❌❌❌


恢复子场景B: 只恢复 Slave B1 (灾难!)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Slave B1 启动:
1. 无持久化文件,启动后为空 ❌

2. 发起选举并晋升为主节点

3. 但数据全部丢失! ❌❌❌

灾难结果: 该槽位段所有数据永久丢失 ❌❌❌


恢复子场景C: 同时恢复 (灾难!)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

无论谁先启动,谁成为主节点:
- 两者都没有持久化数据
- 两者都是空的
- 槽位 5461-10922 的所有数据永久丢失 ❌❌❌

集群架构恢复: ✅
但数据全部丢失: ❌❌❌
```

---

## 五、综合对比表

### 5.1 数据丢失对比

```
┌─────────────────────────────────────────────────────────────────────┐
│                      数据丢失综合对比表                                │
└─────────────────────────────────────────────────────────────────────┘

┌──────────────────┬──────────────────┬──────────────────────────────┐
│ 场景             │ 开启持久化       │ 未开启持久化                      │
│                  │ (appendfsync     │                              │
│                  │  everysec)       │                              │
├──────────────────┼──────────────────┼──────────────────────────────┤
│ 场景1:           │ ~1秒数据 ⚠️      │ ~10ms数据 ✅                    │
│ 1个主节点宕机    │ (持久化窗口 +    │ (仅复制延迟)                      │  
│ (从节点健康)     │  复制延迟)       │ 【更少丢失!】                     │
│                  │                  │                              │
│ 从节点自动晋升   │ 影响: 极小       │ 影响: 极小                        │
│                  │ 恢复: 自动 ✅    │ 恢复: 自动 ✅                   │
├──────────────────┼──────────────────┼──────────────────────────────┤
│ 场景2:           │ ~1秒数据 ⚠️      │ 所有数据! ❌❌❌                 │
│ 2个主节点宕机    │ (仅持久化窗口)   │ (主节点空启动,                     │
│ (从节点健康)     │                  │  覆盖从节点数据)                 │
│                  │ 影响: 2/3槽位    │ 影响: 2/3槽位                   │
│ 无法自动转移     │ 恢复: 需重启 ⚠️  │ 恢复: 灾难 ❌❌                    │
├──────────────────┼──────────────────┼──────────────────────────────┤
│ 场景3:           │ ~1秒数据 ⚠️      │ 所有数据! ❌❌❌                 │
│ 1主+1从宕机      │ (仅持久化窗口)   │ (该组数据完全丢失)                 │
│ (同一组)         │                  │                               │
│                  │ 影响: 1/3槽位    │ 影响: 1/3槽位                   │
│ 无可用从节点     │ 恢复: 需重启 ⚠️  │ 恢复: 灾难 ❌❌                    │
└──────────────────┴──────────────────┴──────────────────────────────┘

关键洞察:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 场景1 (单主节点宕机):
   无持久化反而更好! 因为从节点有最新副本

2. 场景2和3 (多节点宕机或主从同时宕机):
   持久化至关重要! 否则所有数据永久丢失

3. 持久化的真正价值:
   不是为了单点故障(有从节点顶上)
   而是为了灾难性场景(主从同时失效)
```

### 5.2 集群可用性对比

```
┌─────────────────────────────────────────────────────────────────────┐
│                    集群可用性综合对比表                                │
└─────────────────────────────────────────────────────────────────────┘

┌──────────────┬─────────────┬─────────────┬──────────────────────┐
│ 场景         │ 自动恢复    │ 不可用时间  │ cluster-require-         │
│              │             │             │ full-coverage 影响   │
├──────────────┼─────────────┼─────────────┼──────────────────────┤
│ 场景1:       │ ✅ 是       │ ~15-17秒    │ yes: 全部不可用         │
│ 1主节点宕机  │ (从节点晋升)│             │ no:  2/3可用 ✅          │
├──────────────┼─────────────┼─────────────┼──────────────────────┤
│ 场景2:       │ ❌ 否       │ 直到人工    │ yes: 全部不可用          │
│ 2主节点宕机  │ (票数不足)  │ 恢复节点    │ no:  1/3可用              │
├──────────────┼─────────────┼─────────────┼──────────────────────┤
│ 场景3:       │ ❌ 否       │ 直到人工    │ yes: 全部不可用          │
│ 1主1从宕机   │ (无从节点)  │ 恢复节点    │ no:  2/3可用              │
└──────────────┴─────────────┴─────────────┴──────────────────────┘

cluster-require-full-coverage 配置建议:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
yes (默认):
✅ 适合: 金融、订单等数据完整性要求高的场景
❌ 缺点: 部分节点故障导致全部不可用

no:
✅ 适合: 缓存、推荐等可用性优先的场景
❌ 缺点: 部分数据不可访问,应用需处理错误
```

### 5.3 Key 迁移总结

```
┌─────────────────────────────────────────────────────────────────────┐
│                      Key 迁移情况总结                                 │
└─────────────────────────────────────────────────────────────────────┘

重要结论: 在所有故障场景中,都不涉及 Key 的物理迁移!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

场景1: 单主节点宕机,从节点晋升
┌──────────────────────────────────────────┐
│ 误解: Key 从宕机的主节点"迁移"到从节点        │
│ 实际: 从节点本来就有 Key 的副本              │
│       只是角色变化: 从"备份"变"主要"         │
│                                          │
│ 发生的事情:                                │
│ 1. 槽位归属变更 (配置更新)                   │
│ 2. 客户端重定向 (路由表更新)                  │
│ 3. 没有数据传输!                            │
└──────────────────────────────────────────┘

场景2: 两主节点宕机,无法自动恢复
┌──────────────────────────────────────────┐
│ Key 状态:                                │
│ - 停留在各自节点的内存/磁盘中                │
│ - 槽位归属不变                             │
│ - 没有任何迁移发生                          │
│                                          │
│ 恢复后:                                   │
│ - 主节点使用自己的持久化文件                 │
│ - 或从节点使用自己的副本数据                 │
│ - 仍然没有迁移!                            │
└──────────────────────────────────────────┘

场景3: 主从同时宕机
┌──────────────────────────────────────────┐
│ Key 状态:                                 │
│ - 该组的 Key 不可访问                      │
│ - 槽位归属不变(仍属于该组)                  │
│ - 没有迁移到其他节点                        │
│                                          │
│ 恢复后:                                   │
│ - 使用持久化文件恢复                        │
│ - 或数据永久丢失(无持久化)                   │
│ - 不涉及从其他节点"拉取"数据                 │
└──────────────────────────────────────────┘

真正发生 Key 迁移的场景:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
只有在以下情况才会发生物理迁移:
1. 手动执行集群重新分片 (CLUSTER SETSLOT, MIGRATE 命令)
2. 添加新节点并重新分配槽位
3. 移除节点并重新分配槽位

故障恢复不会触发 Key 迁移!
```

---

## 六、生产环境最佳实践

```
┌─────────────────────────────────────────────────────────────────────┐
│                      生产环境配置建议                                  │
└─────────────────────────────────────────────────────────────────────┘

配置组合建议:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

方案1: 数据重要,不能丢失 (推荐金融、订单系统)
┌──────────────────────────────────────────┐
│ 持久化配置:                                │
│ appendonly yes                           │
│ appendfsync everysec                     │
│ save 900 1                               │
│ save 300 10                              │
│ save 60 10000                            │
│                                          │
│ 集群配置:                                 │
│ cluster-require-full-coverage yes        │
│ cluster-node-timeout 15000               │
│                                          │
│ 架构:                                    │
│ 5主5从 (至少)                             │
│ 每个主节点 2 个从节点 (推荐)                │
│ 跨机房部署                                │
│                                          │
│ 特点:                                    │
│ ✅ 数据最安全                             │
│ ✅ 可容忍 2 节点同时故障                    │
│ ⚠️ 性能略有损失 (fsync 开销)                │
│ ⚠️ 部分故障时整体不可用                      │
└──────────────────────────────────────────┘


方案2: 高可用优先,容忍少量数据丢失 (推荐缓存、会话)
┌──────────────────────────────────────────┐
│ 持久化配置:                                │
│ appendonly yes                           │
│ appendfsync everysec                     │
│ (RDB 可选)                               │
│                                          │
│ 集群配置:                                 │
│ cluster-require-full-coverage no ✅      │
│ cluster-node-timeout 15000               │
│                                          │
│ 架构:                                    │
│ 3主3从                                   │
│ 跨机架部署                                │
│                                          │
│ 特点:                                    │
│ ✅ 部分节点故障时其他仍可用                  │
│ ✅ 更高的整体可用性                         │
│ ⚠️ 数据完整性降低                          │
│ ⚠️ 应用需处理部分数据不可用                  │
└──────────────────────────────────────────┘


方案3: 纯缓存场景,性能优先 (谨慎使用)
┌──────────────────────────────────────────┐
│ 持久化配置:                                │
│ appendonly no                            │
│ save ""                                  │
│                                          │
│ 集群配置:                                  │
│ cluster-require-full-coverage no         │
│                                          │
│ 架构:                                    │
│ 3主3从 (最少)                              │
│ 主从物理隔离 ✅✅✅ (关键!)                  │
│                                          │
│ 特点:                                    │
│ ✅ 性能最佳                               │
│ ✅ 单点故障影响最小 (从节点有数据)            │
│ ❌ 主从同时故障 = 数据永久丢失               │
│ ⚠️ 仅适合可重建的缓存数据                    │
└──────────────────────────────────────────┘
```

### 部署拓扑建议

```
┌─────────────────────────────────────────────────────────────────────┐
│                      物理部署最佳实践                                  │
└─────────────────────────────────────────────────────────────────────┘

❌ 错误部署: 主从在同一物理机/机架
┌────────────────────────────┐
│ 物理服务器 1:                │
│ ├─ Master A                │
│ └─ Slave A1  ❌            │
│                            │
│ 问题: 服务器宕机 →            │
│      主从同时失效 ❌❌        │
└────────────────────────────┘


✅ 正确部署: 跨机架/机房隔离
┌────────────────────────────────────────────────┐
│ 机房 A, 机架 1:         机房 B, 机架 2:          │
│ ├─ Master A            ├─ Master B            │
│ ├─ Slave B1            ├─ Slave C1            │
│ └─ Slave C2            └─ Slave A1            │
│                                               │
│ 机房 C, 机架 3:                                │
│ ├─ Master C                                   │
│ ├─ Slave A2                                   │
│ └─ Slave B2                                   │
│                                               │
│ 优点:                                          │
│ - 任意机房/机架故障不影响任何主从组                 │
│ - 最大化容错能力                                 │
└────────────────────────────────────────────────┘


推荐: 每个主节点配置 2 个从节点
┌────────────────────────────────────────────────┐
│ Master A                                       │
│ ├─ Slave A1 (机房 A)                            │
│ └─ Slave A2 (机房 B)                            │
│                                                │
│ 优点:                                           │
│ - 一个从节点宕机,还有备份                          │
│ - 主节点宕机,两个从节点竞选,成功率高                 │
│ - 提高读性能 (负载均衡)                           │
└────────────────────────────────────────────────┘
```

### 监控告警配置

```
┌─────────────────────────────────────────────────────────────────────┐
│                      监控和告警建议                                   │
└─────────────────────────────────────────────────────────────────────┘

关键监控指标:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. 节点存活状态
   - 主节点离线: 🚨🚨🚨 紧急告警 (立即处理)
   - 从节点离线: ⚠️ 警告告警 (1小时内处理)
   - 触发: 立即

2. 主从复制延迟
   - master_repl_offset - slave_repl_offset
   - < 1MB:   正常 ✅
   - 1-10MB:  警告 ⚠️
   - > 10MB:  严重 🚨
   - 触发: 持续 30 秒

3. 槽位覆盖状态
   - cluster_slots_ok != 16384
   - 触发: 立即 🚨🚨🚨
   - 意味着集群部分不可用

4. 集群状态
   - cluster_state != ok
   - 触发: 立即 🚨🚨🚨
   - 整个集群可能不可用

5. AOF 持久化状态
   - aof_last_bgrewrite_status != ok
   - aof_last_write_status != ok
   - 触发: 立即 🚨
   - 持久化失败,数据有风险

6. 内存使用
   - used_memory_rss / maxmemory > 0.8
   - 触发: 警告 ⚠️
   - 可能导致 OOM

7. 慢查询
   - slowlog_len > 100
   - 触发: 警告 ⚠️
   - 性能问题


告警通道建议:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
紧急告警 🚨🚨🚨:
- 短信 + 电话 + 企业微信/钉钉
- 7x24 响应
- 5 分钟内确认

严重告警 🚨:
- 短信 + 企业微信/钉钉
- 工作时间响应
- 30 分钟内确认

警告告警 ⚠️:
- 企业微信/钉钉 + 邮件
- 工作时间处理
- 1 小时内查看
```

### 故障演练建议

```
┌─────────────────────────────────────────────────────────────────────┐
│                      定期故障演练                                    │
└─────────────────────────────────────────────────────────────────────┘

建议每季度进行以下演练:

演练1: 单主节点故障
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
操作: kill -9 一个主节点进程
验证:
✅ 从节点自动晋升 (15-17秒)
✅ 集群状态恢复 OK
✅ 数据丢失量 < 预期
✅ 告警及时触发
✅ 原主节点恢复后自动降级


演练2: 单从节点故障
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
操作: kill -9 一个从节点进程
验证:
✅ 集群仍正常服务
✅ 主节点标记从节点为 FAIL
✅ 告警触发
✅ 从节点恢复后自动重新同步


演练3: 主从同时故障 (最重要!)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
操作: 同时 kill 同一组的主从节点
验证:
✅ 集群进入 CLUSTERDOWN (如果 full-coverage=yes)
✅ 紧急告警触发
✅ 手动恢复流程清晰
✅ 数据丢失量 < 预期
✅ 恢复时间 < SLA


演练4: 机房级故障
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
操作: 断开整个机房的网络
验证:
✅ 其他机房的节点继续服务
✅ 多数派仍可用
✅ 部分槽位可能不可用
✅ 网络恢复后自动重新加入


演练5: 脑裂场景
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
操作: 网络分区,观察行为
验证:
✅ 少数派停止写入 (如配置 min-replicas)
✅ 多数派继续服务
✅ 网络恢复后数据一致性


演练记录模板:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 演练时间: YYYY-MM-DD HH:MM
2. 演练场景: 主从同时故障
3. 操作步骤: ...
4. 预期结果: ...
5. 实际结果: ...
6. 数据丢失: X 条 Key, Y MB
7. 恢复时间: Z 秒
8. 问题发现: ...
9. 改进措施: ...
```

---

## 七、总结

```
┌─────────────────────────────────────────────────────────────────────┐
│                      核心要点总结                                     │
└─────────────────────────────────────────────────────────────────────┘

关于 Key 迁移:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
❌ 误解: 节点故障会触发 Key 物理迁移
✅ 实际: 从节点本来就有数据副本,只是角色变化
        槽位归属变更 ≠ Key 迁移
        真正的 Key 迁移只发生在手动重新分片时


关于数据丢失:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 单主节点宕机 (从节点晋升):
   - 有持久化: 丢失 ~1 秒数据 (持久化+复制延迟)
   - 无持久化: 丢失 ~10ms 数据 (仅复制延迟) ✅ 更好!

2. 多主节点宕机 / 主从同时宕机:
   - 有持久化: 丢失 ~1 秒数据 (仅持久化延迟)
   - 无持久化: 所有数据永久丢失! ❌❌❌

结论: 持久化的价值在于灾难场景,不是单点故障


关于集群可用性:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 单主节点宕机: 自动恢复 ✅ (~15秒)
2. 多主节点宕机: 无法自动恢复 ❌ (票数不足)
3. 主从同时宕机: 无法自动恢复 ❌ (无可用从节点)

多数派原则 (Quorum) 是关键:
- 3主集群: 最多容忍 1 个主节点故障
- 5主集群: 最多容忍 2 个主节点故障


关于持久化策略:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
appendfsync always:   最安全,但性能差
appendfsync everysec: 推荐 ✅ 平衡性能和安全
appendfsync no:       最快,但风险高
无持久化:             仅适合纯缓存 + 主从物理隔离


最重要的防护措施:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 主从物理隔离 🚨🚨🚨 (最关键!)
   - 不同物理机
   - 不同机架
   - 不同机房 (推荐)

2. 开启持久化 🚨🚨 (除非纯缓存)
   - appendonly yes
   - appendfsync everysec

3. 多从节点 🚨
   - 每个主节点至少 1 个从节点
   - 推荐 2 个从节点

4. 监控告警 🚨
   - 节点存活
   - 复制延迟
   - 槽位覆盖
   - 集群状态

5. 定期演练 ⚠️
   - 验证自动故障转移
   - 验证数据恢复
   - 验证告警机制


生产环境推荐配置:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
架构: 5主5从 (或更多)
持久化: appendfsync everysec
集群: cluster-require-full-coverage yes (数据重要)
      cluster-require-full-coverage no (高可用优先)
部署: 跨机房,主从物理隔离
监控: 全方位监控 + 及时告警
演练: 每季度一次


最危险的组合 ⚠️⚠️⚠️:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
主从同时宕机 + 无持久化 = 数据永久丢失! ❌❌❌
```
