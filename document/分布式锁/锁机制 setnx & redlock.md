## setnx & redlock
### 1.为什么需要分布式锁？
与分布式锁相对应的是「单机锁」，我们在写多线程程序时，避免同时操作一个共享变量产生数据问题，通常会使用一把锁来「互斥」，
以保证共享变量的正确性，其使用范围是在「同一个进程」中

如果换做是多个进程，需要同时操作一个共享资源，如何互斥呢？

现在的业务应用通常都是微服务架构，这也意味着一个应用会部署多个进程，那这多个进程如果需要修改 MySQL 中的同一行记录时，
为了避免操作乱序导致数据错误，此时，我们就需要引入「分布式锁」来解决这个问题了。

tips:
> redis是以单线程的形式运行的, redis中也是有事务的，不过这个事务没有mysql中的完善，只保证了一致性和隔离性, 不满足原子性和持久性。

### 2.分布式锁怎么实现？
我们可以使用 SETNX 命令，这个命令表示SET if Not eXists，即如果 key 不存在，才会设置它的值，否则什么也不做。
````
127.0.0.1:6379> SETNX name will // 客户端 1 申请加锁，加锁成功：
(integer) 1
127.0.0.1:6379> SETNX name will // 客户端 2 申请加锁，因为它后到达，加锁失败：
(integer) 0
````
操作完成后，还要及时释放锁，给后来者让出操作共享资源的机会。如何释放锁呢？也很简单，直接使用 DEL 命令删除这个 key 即可
````
127.0.0.1:6379> del name
(integer) 1
````
但是，它存在一个很大的问题，当客户端 1 拿到锁后，如果发生下面的场景，就会造成「死锁」：
- 程序处理业务逻辑异常，没及时释放锁
- 进程挂了，没机会释放锁
这时，这个客户端就会一直占用这个锁，而其它客户端就「永远」拿不到这把锁了。也就产生了死锁问题

### 3.如何避免死锁？
我们很容易想到的方案是，在申请锁时，给这把锁设置一个过期时间
````
127.0.0.1:6379> SETNX lock 1    // 加锁
(integer) 1
127.0.0.1:6379> EXPIRE lock 10  // 10s后自动过期
(integer) 1
````
但是这样还是有问题: 现在的操作，加锁、设置过期是 2 条命令，有没有可能只执行了第一条，第二条却「来不及」执行的情况发生呢？
- SETNX 执行成功，执行 EXPIRE 时由于网络问题，执行失败
- SETNX 执行成功，Redis 异常宕机，EXPIRE 没有机会执行
- SETNX 执行成功，客户端异常崩溃，EXPIRE 也没有机会执行

在 Redis 2.6.12 之后，Redis 扩展了 SET 命令的参数，用这一条命令就可以了：
````
127.0.0.1:6379> SET lock 1 EX 10 NX
OK
````
但是还是有问题的:
- 客户端 1 加锁成功，开始操作共享资源
- 客户端 1 操作共享资源的时间，「超过」了锁的过期时间，锁被「自动释放」
- 客户端 2 加锁成功，开始操作共享资源
- 客户端 1 操作共享资源完成，释放锁（但释放的是客户端 2 的锁）

看到了么，这里存在两个严重的问题：
- 锁过期：客户端 1 操作共享资源耗时太久，导致锁被自动释放，之后被客户端 2 持有
- 释放别人的锁：客户端 1 操作共享资源完成后，却又释放了客户端 2 的锁
### 4.锁被别人释放怎么办(暂不考虑过期问题)?
解决办法是：客户端在加锁时，设置一个只有自己知道的「唯一标识」进去, 如当前的线程id
````
// 锁的VALUE设置为UUID
127.0.0.1:6379> SET lock $uuid EX 20 NX
OK
````
在释放锁时，要先判断这把锁是否还归自己持有
````
if redis.get("lock") == $uuid {
    redis.del("lock")
}
````

这里释放锁使用的是 GET + DEL 两条命令，这时，又会遇到我们前面讲的原子性问题了?
> 我们可以把这个逻辑，写成 Lua 脚本，让 Redis 来执行, 因为 Redis 处理每一个请求是「单线程」执行的，在执行一个 Lua 脚本时，
> 其它请求必须等待，直到这个 Lua 脚本处理完成

这里我们先小结一下，基于 Redis 实现的分布式锁，一个严谨的的流程如下：
- 加锁：`SET lock_key $unique_id EX $expire_time NX`
- 操作共享资源
- 释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁
### 5.锁过期时间不好评估怎么办？
我们能想到的办法就是让时间尽量冗余, 降低提前过期的概率

这个方式不是很好,怎么解决呢?
> 加锁时，先设置一个过期时间，然后我们开启一个「守护线程」，定时去检测这个锁的失效时间，如果锁快要过期了，操作共享资源还未完成，
> 那么就自动对锁进行「续期」，重新设置过期时间

那么单机情况下, 使用 Redis 实现分布式锁所对应的问题以及解决方案:
- 死锁：设置过期时间
- 过期时间评估不好，锁提前过期：守护线程，自动续期
- 锁被别人释放：锁写入唯一标识，释放锁先检查标识，再释放

### 6. [RedLock的由来](https://redis.io/topics/distlock)
我们在使用 Redis 时，一般会采用主从集群 + 哨兵的模式部署，这样做的好处在于，当主库异常宕机时，哨兵可以实现「故障自动切换」，把从库提升为主库，
继续提供服务，以此保证可用性

那当「主从发生切换」时，这个分布锁会依旧安全吗？ 试想这样的场景：
- 客户端 1 在主库上执行 SET 命令，加锁成功
- 此时，主库异常宕机，SET 命令还未同步到从库上（主从复制是异步的）
- 从库被哨兵提升为新主库，这个锁在新的主库上，丢失了！

Redis 的作者提出一种解决方案，就是我们经常听到的 Redlock(红锁)

Redlock 的方案基于 2 个前提：
- 不再需要部署从库和哨兵实例，只部署主库
- 但主库要部署多个，官方推荐至少 5 个实例

> 也就是说，想用使用 Redlock，你至少要部署 5 个 Redis 实例，而且都是主库，它们之间没有任何关系，都是一个个孤立的实例。
>
> 不是部署 Redis Cluster，就是部署 5 个简单的 Redis 实例。

Redlock 具体如何使用呢?
- 客户端获取当前的时间戳 T1
- 客户端一次向 5 个 Redis 实例发起加锁请求(set lock_key $unique_id EX $expire_time NX), 并为每个请求设置超时时间(毫秒级, 远小于锁的有效时间),  
    如果一个实例加锁失败(网络超时等各种异常情况), 就立刻向下一个Redis实例发起请求
- 如果客户端从 >=3 个（大多数）以上 Redis 实例加锁成功，则再次获取「当前时间戳T2」，如果 T2 - T1 < 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败
- 加锁成功，去操作共享资源
- 锁失败，向「全部节点」发起释放锁请求（Lua 脚本释放锁, get(lock_key) == uuid { del lock_key }）

以上有四个重点:
- 客户端在多个 Redis 实例上申请加锁
- 必须保证大多数节点加锁成功
- 大多数节点加锁的总耗时，要小于锁设置的过期时间
- 释放锁，要向全部节点发起释放锁请求

#### 6.1 为什么要在多个实例上加锁？
本质上是为了「容错」，部分实例异常宕机，剩余的实例加锁成功，整个锁服务依旧可用。
#### 6.2 为什么大多数加锁成功，才算成功？
多个 Redis 实例一起来用，其实就组成了一个「分布式系统」。
在分布式系统中，总会出现「异常节点」，所以，在谈论分布式系统问题时，需要考虑异常节点达到多少个，也依旧不会影响整个系统的「正确性」。
这是一个分布式系统「容错」问题，这个问题的结论是：如果只存在「故障」节点，只要大多数节点正常，那么整个系统依旧是可以提供正确服务的
#### 6.3 为什么步骤 3 加锁成功后，还要计算加锁的累计耗时？
因为操作的是多个节点，所以耗时肯定会比操作单个实例耗时更久，而且，因为是网络请求，网络情况是复杂的，有可能存在延迟、丢包、超时等情况发生，网络请求越多，异常发生的概率就越大。
所以，即使大多数节点加锁成功，但如果加锁的累计耗时已经「超过」了锁的过期时间，那此时有些实例上的锁可能已经失效了，这个锁就没有意义了。

### 使用 RedLock 的问题(简略)
- 作者同意对方关于「时钟跳跃」对 Redlock 的影响，但认为时钟跳跃是可以避免的，取决于基础设施和运维。
- Redlock 在设计时，充分考虑了 NPC 问题，在 Redlock 步骤 3 之前出现 NPC，可以保证锁的正确性，但在步骤 3 之后发生 NPC，
    不止是 Redlock 有问题，其它分布式锁服务同样也有问题，所以不在讨论范畴内。

> Redlock 的个人看法是，尽量不用它，而且它的性能不如单机版 Redis，部署成本也高，我还是会优先考虑使用主从+ 哨兵的模式 实现分布式锁

[K神链接](https://mp.weixin.qq.com/s/s8xjm1ZCKIoTGT3DCVA4aw)
